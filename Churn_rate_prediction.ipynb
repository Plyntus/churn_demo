{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivin R. Homework. \n",
    "## Churn analysis\n",
    "### HSE, Data-driven 21, May'22\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-work <a class=\"anchor\" id=\"pre\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage we load all packages required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install -c conda-forge xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np # Packages for data wrangling and modelling\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns # Packages for visualization\n",
    "\n",
    "from xgboost import XGBClassifier \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, average_precision_score\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold, RandomizedSearchCV \n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score, cross_validate\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/rostislav/opt/anaconda3/notebooks/HSE Data-driven/Datasets/WA_Fn-UseC_-Telco-Customer-Churn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation and exploration <a class=\"anchor\" id=\"expl\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.customerID.nunique() == len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No duplicates allready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Churn.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample is unbalanced!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for column in df.columns:\n",
    "#    print(df[column].value_counts())\n",
    "#    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.TotalCharges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = []\n",
    "for n,value in enumerate(df.TotalCharges):\n",
    "    try: float(value)\n",
    "    except: to_drop.append(n)\n",
    "df = df.drop(df.index[to_drop])\n",
    "df.TotalCharges = df.TotalCharges.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nominal features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features \"Internet service\" and \"Phone Service\" are detailed in few others. E.g. Phone service detailed in the \"Multiple lines\". They, by definitions, would be highly correlated which will make the regression is statistically insignificant. So, we should either keep only the main features or the detailed ones. Let's try both approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['customerID'],axis=1)\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main = df.drop(['MultipleLines','OnlineSecurity','OnlineBackup',\n",
    "                   'DeviceProtection','TechSupport','StreamingTV',\n",
    "                   'StreamingMovies'],axis=1)\n",
    "df_detailed = df.drop(['PhoneService','InternetService'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main = df_main.replace('No',0)\n",
    "df_main = df_main.replace('Yes',1)\n",
    "\n",
    "df_detailed = df_detailed.replace('No',0)\n",
    "df_detailed = df_detailed.replace('Yes',1)\n",
    "df_detailed = df_detailed.replace('No internet service',0)\n",
    "df_detailed = df_detailed.replace('No phone service',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_detailed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies_main = pd.get_dummies(df[['gender','InternetService','Contract','PaymentMethod']])\n",
    "df_main = pd.concat([df_main, dummies_main], axis=1)\n",
    "df_main = df_main.drop(['gender','InternetService','Contract','PaymentMethod'],axis=1)\n",
    "\n",
    "dummies_detailed = pd.get_dummies(df[['gender','Contract','PaymentMethod']])\n",
    "df_detailed = pd.concat([df_detailed, dummies_detailed], axis=1)\n",
    "df_detailed = df_detailed.drop(['gender','Contract','PaymentMethod'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multicollinearity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(df_main.corr())\n",
    "corr_matrix = df_main.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(df_detailed.corr())\n",
    "corr_matrix = df_detailed.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything is ok, we can proceed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGboost classifier implementation <a class=\"anchor\" id=\"model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regressors and key variable\n",
    "X = df_main.drop(\"Churn\",axis=1)\n",
    "y = df_main.Churn\n",
    "\n",
    "#train-test\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25)\n",
    "\n",
    "#shuffling\n",
    "data = df_main.sample(frac=1)\n",
    "X = df_main.drop(\"Churn\",axis=1)\n",
    "y = df_main.Churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model \n",
    "model = XGBClassifier()\n",
    "\n",
    "#parameters\n",
    "param_list = {\n",
    "    'silent': [False],\n",
    "    'max_depth': range(4,40),\n",
    "    'learning_rate': [0.005, 0.01, 0.1, 0.2],\n",
    "    'subsample': np.arange(0,1.2,.3),\n",
    "    'colsample_bytree': np.arange(0,1.2,.3),\n",
    "    'colsample_bylevel': np.arange(0,1.2,.3),\n",
    "    'min_child_weight': [0.5, 1.0, 2.0],\n",
    "    'gamma': [0, 0.25, 0.5, 0.75, 1.0],\n",
    "    'reg_lambda': [0.1, 10.0, 50.0, 100.0, 500.0, 1000.0],\n",
    "    'n_estimators': [10, 50],\n",
    "    'scale_pos_weight': [1, 6],\n",
    "    'max_delta_step': [1, 2, 3]\n",
    "}\n",
    "kfold = 10\n",
    "cv_strat = RepeatedStratifiedKFold(n_splits=kfold,n_repeats=5)\n",
    "\n",
    "#Randomized Search\n",
    "cv = RandomizedSearchCV(model,param_list,cv=cv_strat,n_iter=10,verbose=1,scoring=\"balanced_accuracy\",n_jobs=-1).fit(X.values,y.values)\n",
    "\n",
    "model_best = cv.best_estimator_\n",
    "cv.best_params_ # To see best hyperparameters found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = model_best.predict(X_train.values)\n",
    "y_pred = model_best.predict(X_test.values)\n",
    "\n",
    "cv_strat = RepeatedStratifiedKFold(n_splits=kfold,n_repeats=20)\n",
    "scores = cross_validate(model_best,X.values,y.values,cv=cv_strat,verbose=3,n_jobs=-1,return_train_score=True,\n",
    "                        scoring={\"roc_auc\":\"roc_auc\",\n",
    "                                 \"recall\":\"recall\",\n",
    "                                 \"precision\":\"precision\",\n",
    "                                 \"accuracy\":\"accuracy\",\n",
    "                                 \"balanced_accuracy\":\"balanced_accuracy\",\n",
    "                                 \"average_precision\":\"average_precision\"}) \n",
    "\n",
    "stat_xgb = pd.DataFrame(pd.DataFrame(scores).mean(),columns=[\"Score_main\"]).drop([\"fit_time\",\"score_time\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a confusion matrix to visualise precision, recall, misclassification and false alarms\n",
    "cm = pd.DataFrame(confusion_matrix(y_test, y_pred), index = list(set(y)), columns = list(set(y)))\n",
    "\n",
    "#visualise the confusion matrix\n",
    "plt.figure()\n",
    "sns.heatmap(cm, annot = True, fmt=\"d\",\n",
    "            cmap=sns.color_palette(\"Blues\")).set(xlabel='predicted values', \n",
    "                                                ylabel='real values', \n",
    "                                                title = 'Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the metrics are ok, from confusion matrix we can see that model often predicts churn mistakenly "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feat_imp = pd.DataFrame(list(zip(data.columns,model_best.feature_importances_)),columns=[\"Feature\",\"Importance\"]).sort_values(by=\"Importance\",ascending=False)\n",
    "fig_store = plt.figure(figsize=(10,20))\n",
    "sns.barplot(y=\"Feature\",x=\"Importance\",data = feat_imp,orient=\"h\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see, that only limited number of features is relevant for this model. Let's compare with the other model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regressors and key variable\n",
    "X = df_detailed.drop(\"Churn\",axis=1)\n",
    "y = df_detailed.Churn\n",
    "\n",
    "#train-test\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25)\n",
    "\n",
    "#shuffling\n",
    "data = df_detailed.sample(frac=1)\n",
    "X = df_detailed.drop(\"Churn\",axis=1)\n",
    "y = df_detailed.Churn\n",
    "\n",
    "#model \n",
    "model = XGBClassifier()\n",
    "\n",
    "#parameters\n",
    "param_list = {\n",
    "    'silent': [False],\n",
    "    'max_depth': range(4,40),\n",
    "    'learning_rate': [0.005, 0.01, 0.1, 0.2],\n",
    "    'subsample': np.arange(0,1.2,.3),\n",
    "    'colsample_bytree': np.arange(0,1.2,.3),\n",
    "    'colsample_bylevel': np.arange(0,1.2,.3),\n",
    "    'min_child_weight': [0.5, 1.0, 2.0],\n",
    "    'gamma': [0, 0.25, 0.5, 0.75, 1.0],\n",
    "    'reg_lambda': [0.1, 10.0, 50.0, 100.0, 500.0, 1000.0],\n",
    "    'n_estimators': [10, 50],\n",
    "    'scale_pos_weight': [1, 6],\n",
    "    'max_delta_step': [1, 2, 3]\n",
    "}\n",
    "kfold = 10\n",
    "cv_strat = RepeatedStratifiedKFold(n_splits=kfold,n_repeats=5)\n",
    "\n",
    "#Randomized Search\n",
    "cv = RandomizedSearchCV(model,param_list,cv=cv_strat,n_iter=10,verbose=1,scoring=\"balanced_accuracy\",n_jobs=-1).fit(X.values,y.values)\n",
    "\n",
    "model_best = cv.best_estimator_\n",
    "\n",
    "y_pred_train = model_best.predict(X_train.values)\n",
    "y_pred = model_best.predict(X_test.values)\n",
    "\n",
    "cv_strat = RepeatedStratifiedKFold(n_splits=kfold,n_repeats=20)\n",
    "scores = cross_validate(model_best,X.values,y.values,cv=cv_strat,verbose=3,n_jobs=-1,return_train_score=True,\n",
    "                        scoring={\"roc_auc\":\"roc_auc\",\n",
    "                                 \"recall\":\"recall\",\n",
    "                                 \"precision\":\"precision\",\n",
    "                                 \"accuracy\":\"accuracy\",\n",
    "                                 \"balanced_accuracy\":\"balanced_accuracy\",\n",
    "                                 \"average_precision\":\"average_precision\"}) \n",
    "\n",
    "stat_xgb_detailed = pd.DataFrame(pd.DataFrame(scores).mean(),columns=[\"Score\"]).drop([\"fit_time\",\"score_time\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_xgb['Score_detaile'] = stat_xgb_detailed.Score\n",
    "stat_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a confusion matrix to visualise precision, recall, misclassification and false alarms\n",
    "cm = pd.DataFrame(confusion_matrix(y_test, y_pred), index = list(set(y)), columns = list(set(y)))\n",
    "\n",
    "#visualise the confusion matrix\n",
    "plt.figure()\n",
    "sns.heatmap(cm, annot = True, fmt=\"d\",\n",
    "            cmap=sns.color_palette(\"Blues\")).set(xlabel='predicted values', \n",
    "                                                ylabel='real values', \n",
    "                                                title = 'Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp = pd.DataFrame(list(zip(data.columns,model_best.feature_importances_)),columns=[\"Feature\",\"Importance\"]).sort_values(by=\"Importance\",ascending=False)\n",
    "fig_store = plt.figure(figsize=(10,20))\n",
    "sns.barplot(y=\"Feature\",x=\"Importance\",data = feat_imp,orient=\"h\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Detailed features enable much more important regressors, but each of them has limited importance. The model in general works significantly worse than general model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After modelling we discover that it's almost vital for telecom to convince its customers to become their internet service provider. This is the most viable predictor of the churn. \n",
    "\n",
    "Among other features:\n",
    "- the longer the contract the better\n",
    "- Billing via internet is the best (and good for Earth)\n",
    "- Gender has medium importance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
